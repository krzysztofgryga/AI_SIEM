# AI Monitoring POC Configuration

# =================================================================
# LOCAL LLM ENDPOINTS (NO API KEYS NEEDED!)
# =================================================================
# Use these for completely free, local AI models

# Ollama (recommended for local use)
# Install: https://ollama.ai/download
# Start: ollama serve
# Pull model: ollama pull llama2
OLLAMA_HOST=http://localhost:11434

# LM Studio
# Download: https://lmstudio.ai/
# Start local server on port 1234
LM_STUDIO_HOST=http://localhost:1234

# LocalAI
# Docker: docker run -p 8080:8080 quay.io/go-skynet/local-ai:latest
LOCALAI_HOST=http://localhost:8080

# =================================================================
# CLOUD API KEYS (OPTIONAL)
# =================================================================
# Only needed if you want to use cloud-based APIs

# OpenAI API Key
# Get yours at: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic API Key
# Get yours at: https://console.anthropic.com/settings/keys
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# =================================================================
# GENERAL SETTINGS
# =================================================================

# User identification
DEFAULT_USER_ID=local_user

# Database path
DATABASE_PATH=ai_monitoring.db

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO
