# Docker Quick Start - 2 minuty do dzia≈ÇajƒÖcego systemu! üöÄ

## Automatyczny Setup (Najprostsze)

```bash
# 1. Uruchom setup
make setup

# 2. Gotowe! üéâ
```

To wszystko! Skrypt:
- Uruchomi Ollama
- Pobierze model (llama2/tinyllama/mistral)
- Uruchomi monitoring
- Poka≈ºe przyk≈Çady u≈ºycia

---

## Rƒôczny Setup (3 komendy)

```bash
# 1. Uruchom serwisy
make up

# 2. Pobierz model
make pull-llama2

# 3. Testuj
make test
```

---

## Podstawowe Komendy

```bash
# Uruchomienie
make up              # Start podstawowych serwis√≥w
make up-full         # Start wszystkiego (+ UI)

# Testowanie
make test            # Uruchom przyk≈Çad
make cli             # Dashboard CLI
make stats           # Poka≈º statystyki

# ZarzƒÖdzanie
make logs            # Zobacz logi
make shell           # Wejd≈∫ do kontenera
make down            # Zatrzymaj wszystko

# Modele
make pull-llama2     # Pobierz llama2
make pull-tinyllama  # Pobierz tinyllama (ma≈Çy, szybki)
make list-models     # Poka≈º pobrane modele

# Pomoc
make help            # Wszystkie komendy
```

---

## Przyk≈Çady U≈ºycia

### 1. Szybki test
```bash
make test
```

Output:
```
ü¶ô Ollama Example

Connecting to Ollama at http://ollama:11434...

1. Simple generation with llama2
üìù Event: llama2 - 28 tokens - 1234ms
Response: 2 + 2 equals 4.
```

### 2. Dashboard
```bash
make cli
```

Interaktywne menu:
```
Choose an option:

1. View Statistics
2. View Recent Events
3. View Recent Anomalies
4. View High Risk Events
5. Exit

Enter choice (1-5):
```

### 3. Custom script
```bash
# Wejd≈∫ do kontenera
make shell

# Stw√≥rz sw√≥j skrypt
cat > my_test.py <<'EOF'
import asyncio
from local_collector import OllamaCollector

async def main():
    collector = OllamaCollector(base_url="http://ollama:11434")
    result = await collector.generate(
        model="llama2",
        prompt="Write a haiku about AI"
    )
    print(result['response'])

asyncio.run(main())
EOF

# Uruchom
python my_test.py
```

---

## Dostƒôpne Serwisy

Po `make up`:

| Serwis | URL | Opis |
|--------|-----|------|
| Ollama | http://localhost:11434 | Lokalny LLM server |
| Monitoring | (container) | System monitoringu |

Po `make up-full`:

| Serwis | URL | Opis |
|--------|-----|------|
| Ollama | http://localhost:11434 | Lokalny LLM server |
| LocalAI | http://localhost:8080 | Alternatywny LLM server |
| SQLite UI | http://localhost:8081 | PrzeglƒÖdarka bazy danych |
| Monitoring | (container) | System monitoringu |

---

## Modele Ollama

| Model | Rozmiar | Szybko≈õƒá | Jako≈õƒá | Komenda |
|-------|---------|----------|--------|---------|
| tinyllama | 637MB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | `make pull-tinyllama` |
| llama2 | 3.8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | `make pull-llama2` |
| mistral | 4.1GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | `make pull-mistral` |

**Rekomendacja**:
- S≈Çaby PC: `tinyllama`
- Normalny: `llama2`
- Mocny: `mistral`

---

## Troubleshooting

### Problem: `make: command not found`
```bash
# U≈ºyj bezpo≈õrednio docker-compose
docker-compose up -d
```

### Problem: Ollama nie odpowiada
```bash
# Restart
make restart

# Sprawd≈∫ logi
make logs-ollama
```

### Problem: Wolne odpowiedzi
```bash
# U≈ºyj mniejszego modelu
make pull-tinyllama

# W kodzie zmie≈Ñ na model="tinyllama"
```

### Problem: Brak miejsca
```bash
# Usu≈Ñ nieu≈ºywane modele
docker exec -it ai-monitoring-ollama ollama rm <model-name>

# Wyczy≈õƒá Docker
docker system prune -a
```

---

## Co Dalej?

1. **Przeczytaj pe≈ÇnƒÖ dokumentacjƒô**: [DOCKER_README.md](DOCKER_README.md)
2. **Zobacz przyk≈Çady**: [local_example.py](local_example.py)
3. **G≈Ç√≥wny README**: [README.md](README.md)

---

## Cheat Sheet

```bash
# Kompletny workflow
make setup           # Pierwszy raz
make test            # Test
make stats           # Statystyki
make cli             # Dashboard
make down            # Koniec

# Debug
make logs            # Zobacz co siƒô dzieje
make shell           # Wejd≈∫ do ≈õrodka
make status          # Status serwis√≥w

# Czyszczenie
make clean           # Zatrzymaj
make clean-all       # Usu≈Ñ WSZYSTKO (ostro≈ºnie!)
make backup          # Backup bazy przed usuniƒôciem
```

---

**üéâ To wszystko! Masz dzia≈ÇajƒÖcy system monitoringu AI bez ≈ºadnych API keys!**

Potrzebujesz pomocy? Zobacz:
- `make help` - lista komend
- [DOCKER_README.md](DOCKER_README.md) - pe≈Çna dokumentacja
- [Issues](https://github.com/krzysztofgryga/AI_SIEM/issues) - zg≈Ço≈õ problem
